{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q -U \"tensorflow-text==2.11.*\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
    "\n",
    "dataset_dir = utils.get_file(\n",
    "    origin=data_url,\n",
    "    untar=True,\n",
    "    cache_dir='stack_overflow',\n",
    "    cache_subdir='')\n",
    "\n",
    "dataset_dir = pathlib.Path(dataset_dir).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = dataset_dir/'train'\n",
    "list(train_dir.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set.\n",
    "raw_val_ds = utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = dataset_dir/'test'\n",
    "\n",
    "# Create a test set.\n",
    "raw_test_ds = utils.text_dataset_from_directory(\n",
    "    test_dir,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds = raw_train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "raw_val_ds = raw_val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "raw_test_ds = raw_test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "\n",
    "binary_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "int_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text-only dataset (without labels), then call `TextVectorization.adapt`.\n",
    "train_text = raw_train_ds.map(lambda text, labels: text)\n",
    "binary_vectorize_layer.adapt(train_text)\n",
    "int_vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pydot graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model = tf.keras.Sequential([\n",
    "    binary_vectorize_layer,\n",
    "    layers.Dense(4)])\n",
    "\n",
    "binary_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "tf.keras.utils.plot_model(binary_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_history = binary_model.fit(\n",
    "    raw_train_ds, validation_data=raw_val_ds, epochs=10)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, num_labels, vectorizer=None):\n",
    "  my_layers =[]\n",
    "  if vectorizer is not None:\n",
    "    my_layers = [vectorizer]\n",
    "\n",
    "  my_layers.extend([\n",
    "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_labels)\n",
    "  ])\n",
    "\n",
    "  model = tf.keras.Sequential(my_layers)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `vocab_size` is `VOCAB_SIZE + 1` since `0` is used additionally for padding.\n",
    "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4, vectorizer=int_vectorize_layer)\n",
    "\n",
    "tf.keras.utils.plot_model(int_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "int_history = int_model.fit(raw_train_ds, validation_data=raw_val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_ds = raw_train_ds.map(lambda x,y: (binary_vectorize_layer(x), y))\n",
    "binary_val_ds = raw_val_ds.map(lambda x,y: (binary_vectorize_layer(x), y))\n",
    "binary_test_ds = raw_test_ds.map(lambda x,y: (binary_vectorize_layer(x), y))\n",
    "\n",
    "int_train_ds = raw_train_ds.map(lambda x,y: (int_vectorize_layer(x), y))\n",
    "int_val_ds = raw_val_ds.map(lambda x,y: (int_vectorize_layer(x), y))\n",
    "int_test_ds = raw_test_ds.map(lambda x,y: (int_vectorize_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model.export('bin.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load('bin.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model.predict(['How do you sort a list?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded.serve(tf.constant(['How do you sort a list?'])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "for name in FILE_NAMES:\n",
    "  text_dir = utils.get_file(name, origin=DIRECTORY_URL + name)\n",
    "\n",
    "parent_dir = pathlib.Path(text_dir).parent\n",
    "list(parent_dir.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeler(example, index):\n",
    "  return example, tf.cast(index, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "  lines_dataset = tf.data.TextLineDataset(str(parent_dir/file_name))\n",
    "  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "  labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "\n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tf_text.UnicodeScriptTokenizer()\n",
    "\n",
    "  def call(self, text):\n",
    "    lower_case = tf_text.case_fold_utf8(text)\n",
    "    result = self.tokenizer.tokenize(lower_case)\n",
    "    # If you pass a batch of strings, it will return a RaggedTensor.\n",
    "    if isinstance(result, tf.RaggedTensor):\n",
    "      # Convert to dense 0-padded.\n",
    "      result = result.to_tensor()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = all_labeled_data.map(lambda text, label: (tokenizer(text), label))\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = tokenized_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "vocab_count = collections.Counter()\n",
    "for toks, labels in tokenized_ds.ragged_batch(1000):\n",
    "  toks = tf.reshape(toks, [-1])\n",
    "  for tok in toks.numpy():\n",
    "    vocab_count[tok] += 1\n",
    "\n",
    "vocab = [tok for tok, count in vocab_count.most_common(VOCAB_SIZE)]\n",
    "\n",
    "print(\"First five vocab entries:\", vocab[:5])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVocabTable(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab):\n",
    "    super().__init__()\n",
    "    self.keys = [''] + vocab\n",
    "    self.values = range(len(self.keys))\n",
    "\n",
    "    self.init = tf.lookup.KeyValueTensorInitializer(\n",
    "        self.keys, self.values, key_dtype=tf.string, value_dtype=tf.int64)\n",
    "\n",
    "    num_oov_buckets = 1\n",
    "\n",
    "    self.table = tf.lookup.StaticVocabularyTable(self.init, num_oov_buckets)\n",
    "\n",
    "  def call(self, x):\n",
    "    result = self.table.lookup(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVocabTable(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab):\n",
    "    super().__init__()\n",
    "    self.keys = [''] + vocab\n",
    "    self.values = range(len(self.keys))\n",
    "\n",
    "    self.init = tf.lookup.KeyValueTensorInitializer(\n",
    "        self.keys, self.values, key_dtype=tf.string, value_dtype=tf.int64)\n",
    "\n",
    "    num_oov_buckets = 1\n",
    "\n",
    "    self.table = tf.lookup.StaticVocabularyTable(self.init, num_oov_buckets)\n",
    "\n",
    "  def call(self, x):\n",
    "    result = self.table.lookup(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_table = MyVocabTable(['a','b','c'])\n",
    "vocab_table(tf.constant([''] + list('abcdefghi')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_table = MyVocabTable(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text = tf.keras.Sequential([\n",
    "    tokenizer,\n",
    "    vocab_table\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text, example_label = next(iter(all_labeled_data))\n",
    "print(\"Sentence: \", example_text.numpy())\n",
    "vectorized_text = preprocess_text(example_text)\n",
    "print(\"Vectorized sentence: \", vectorized_text.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encoded_data = all_labeled_data.map(lambda text, labels:(preprocess_text(text), labels))\n",
    "\n",
    "for ids, label in all_encoded_data.take(1):\n",
    "  break\n",
    "\n",
    "print(\"Ids: \", ids.numpy())\n",
    "print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_encoded_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "validation_data = all_encoded_data.take(VALIDATION_SIZE).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text, sample_labels = next(iter(validation_data))\n",
    "print(\"Text batch shape: \", sample_text.shape)\n",
    "print(\"Label batch shape: \", sample_labels.shape)\n",
    "print(\"First text example: \", sample_text[0])\n",
    "print(\"First label example: \", sample_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.prefetch(tf.data.AUTOTUNE)\n",
    "validation_data = validation_data.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(vocab_size=VOCAB_SIZE+2, num_labels=3)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, validation_data=validation_data, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.evaluate(validation_data, return_dict=True)\n",
    "\n",
    "print(\"Loss: \", metrics['loss'])\n",
    "print(\"Accuracy: {:2.2%}\".format(metrics['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = tf.keras.Sequential([\n",
    "    preprocess_text,\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test dataset of raw strings.\n",
    "test_ds = all_labeled_data.take(VALIDATION_SIZE).batch(BATCH_SIZE)\n",
    "test_ds = test_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = export_model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: {:2.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(export_model, 'export.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load('export.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model(tf.constant(['The field bristled with the long and deadly spears which they bore.'])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded(tf.constant(['The field bristled with the long and deadly spears which they bore.'])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"Join'd to th' Ionians with their flowing robes,\",  # Label: 1\n",
    "    \"the allies, and his armour flashed about him so that he seemed to all\",  # Label: 2\n",
    "    \"And with loud clangor of his arms he fell.\",  # Label: 0\n",
    "]\n",
    "\n",
    "predicted_scores = export_model.predict(inputs)\n",
    "predicted_labels = tf.math.argmax(predicted_scores, axis=1)\n",
    "\n",
    "for input, label in zip(inputs, predicted_labels):\n",
    "  print(\"Question: \", input)\n",
    "  print(\"Predicted label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set.\n",
    "train_ds = tfds.load(\n",
    "    'imdb_reviews',\n",
    "    split='train[:80%]',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set.\n",
    "val_ds = tfds.load(\n",
    "    'imdb_reviews',\n",
    "    split='train[80%:]',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Make a text-only dataset (without labels), then call `TextVectorization.adapt`.\n",
    "train_text = train_ds.map(lambda text, labels: text)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(vectorize_text)\n",
    "val_ds = val_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure datasets for performance as before.\n",
    "train_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(vocab_size=VOCAB_SIZE, num_labels=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, validation_data=val_ds, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(val_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: {:2.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = tf.keras.Sequential(\n",
    "    [vectorize_layer, model,\n",
    "     layers.Activation('sigmoid')])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 --> negative review\n",
    "# 1 --> positive review\n",
    "inputs = [\n",
    "    \"This is a fantastic movie.\",\n",
    "    \"This is a bad movie.\",\n",
    "    \"This movie was so bad that it was good.\",\n",
    "    \"I will never say yes to watching this movie.\",\n",
    "]\n",
    "\n",
    "predicted_scores = export_model.predict(inputs)\n",
    "predicted_labels = [int(round(x[0])) for x in predicted_scores]\n",
    "\n",
    "for input, label in zip(inputs, predicted_labels):\n",
    "  print(\"Question: \", input)\n",
    "  print(\"Predicted label: \", label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
